{
  "hash": "542040bdd810f4970571ddff9b020755",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Make More Sense of Language Through Embeddings\"\ndescription: \"Representing texts in a better way that neural networks understand\"\nfrom: markdown+emoji\nexecute:\n  eval: false\nauthor:\n  - name: Koushik Khan \ndate: 05-01-2023\ncategories: [deep learning]\nimage: text.jpg\ndraft: false\n---\n\n<center>\n<img src=\"text.jpg\" alt=\"alt text\" title=\"dfg\" height=\"400\" width=\"800\"/>\n<figcaption>Photo by <a href=\"https://unsplash.com/@melpoole?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Mel Poole</a> on <a href=\"https://unsplash.com/photos/lBsvzgYnzPU?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n  </figcaption>\n</center>\n\n# Introduction\n\nA computer algorithm is nothing but a sequence of carefully chosen steps having a specific goal, it needs some input data to work on and produce some results. Achieving this goal depends on many many factors and representing the input data to the algorithm is one of them.\n\nNeural networks, being some complex algorithms, are capable of performing many useful tasks with human languages when represented both as texts and sound waves. Some useful examples can be *human sentiment detection* from written text, *language translation*, *image captioning* etc. \n\nIn all these cases, one interesting thing is to know how such a massive amount of textual data are supplied to a network so that it understands the inner meaning just like a human.  \n\nIn this post I will focus on a couple of methods to represent a textual dataset to a network, collectively known as word2vec.\n\n# Some important terminologies\n\nLet us understand some basic yet important terminologies that are used while working with textual data. For the sake of convenience, we will consider *English* as the language to understand various concepts in this post, but the same terminologies are applicable for other languages too. \n\n+ **Corpus**: A corpus is a collection of textual data. It typically refers to a group of sentences or paragraphs. For example, the messages we send to someone in any messaging platform can be considered together to form a corpus.\n\n+ **Token**: A token is the most granular part of a corpus. Depending on the requirement, it can be a single character, a single word or even a single sentence. The process of creating tokens out of a corpus is called *tokenization*.\n\n+ **Vocabulary**: A vocabulary is the set of unique (non-repetitive) tokens. Technically, it is that set which generates the entire corpus.\n\nAn example will help here to understand it better.\n\n::: {#54c4d1c6 .cell execution_count=1}\n``` {.python .cell-code}\ncorpus = ['Mathematics is the language to understand the nature']\n\nword_level_tokens = [\n  'Mathematics', 'is', 'the', \n  'language', 'to', 'understand', \n  'the', 'nature'\n]\n\nvocabulary = {'Mathematics', 'is', 'the', 'language', 'to', 'understand', 'nature'}\n```\n:::\n\n\nNote that, the word `the` has appeared twice in the corpus and it is considered only once in the vocabulary.\n\n# Converting texts to numbers\n\nConverting texts to numbers are required because a computer can only handle numbers, although typically not in the form we give. It converts numbers (decimal numbers) to the binary form which is basically a sequence of zeros and ones, before performing any task.\n\nSince the beginning of development of modern computers, we had something called [ASCII table](https://www.ascii-code.com/). This table has numeric representation of each character in English including numbers and some special characters.\n\n# Better ways to represent texts\n\nASCII codes are certainly useful to represent characters in the computers' memory. But it does not fulfill the purpose while developing modern intelligent applications like a language translator.\n\nIn natural language processing, tokens are usually represented by numerical arrays or vectors. Let us understand the various ways to create such vectors.\n\nBefore proceeding further, let us consider a new corpus (a group of sentences quoted by [Shakuntala Devi](https://en.wikipedia.org/wiki/Shakuntala_Devi)) with a word level tokenization as below:\n\n::: {#cd5fd48e .cell execution_count=2}\n``` {.python .cell-code}\ncorpus = [\n  'without mathematics, thereâ€™s nothing you can do',\n  'everything around you is mathematics',\n  'everything around you is number'\n]\n\nword_level_tokens = [\n  \"without\", \"mathematics\", \"there's\",\n  \"nothing\", \"you\", \"can\", \"do\", \n  \"everything\", \"around\", \"you\", \"is\", \n  \"mathematics\", \"everything\", \"around\", \n  \"you\", \"is\", \"number\"\n]\n\nsorted_vocabulary = [\n  'around',\n  'can',\n  'do',\n  'everything',\n  'is',\n  'mathematics',\n  'nothing',\n  'number',\n  \"there's\",\n  'without',\n  'you'\n]\n\nvocabulary_size = 11\n```\n:::\n\n\n## One-hot vector\n\nThis is the simplest way to convert tokens into a vector. One-hot vector, for a token, is a vector of dimension `vocabulary_size` (often denoted by $|V|$). In this vector, all the elements are $0$'s except a single $1$. The index that holds the $1$ is called *hot* index and it is basically the index of the token in the vocabulary.\n\nExample: \n\n::: {#59d70994 .cell execution_count=3}\n``` {.python .cell-code}\none_hot_for_around = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\none_hot_for_can = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\none_hot_for_number = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n```\n:::\n\n\nAs you can imagine, for a large corpus, one-hot vectors are very large dimensional sparse vectors.\n\nAlso, if we represent any text using one-hot vectors (*technically a matrix made of one-hot vectors*), then basically we are not considering word-word relationship or similarity. \n\nWords (rather tokens) are considered independent chunks of the text for this kind of setup, but it is something that we do not like. Any language maintains some contexts through word-word relationships and those are completely ignored by one-hot representation.\n\nIf we try to measure the similarity between two words with the help of one-hot representation, we will always get zero, but that does not mean the words are very close on the semantic space.\n\n## Embedding vector\n\nAs you have observed, one-hot vectors do not consider context, so we definitely need something meaningful for representing the tokens mathematically. \n\nIn the world of NLP, the term *embedding* usually refers to a dense vector (a vector that has most of its components as non-zero real numbers) that represents a token. As you know, depending on the problem, a token can be a character, a word or even a full sentence and an embedding vector is created to represent it accordingly.\n\nLet us try to understand the way of expressing the meaning of words mathematically.\n\n![Figure 1: Word features representation](word-features.svg){ width=60% }\n\nIn the above table, we have four different words which are 'Chocolate', 'Cat', 'Dog' and 'Ship'. For each of these words we have six randomly chosen features or characteristics. \n\nEach of the features can have values in between zero and one. If the value for a feature is very close to zero, it indicates that the feature is not very applicable to the word. Similarly, having a value closer to one indicates that the feature is quite applicable to the word.\n\nAs an example, the feature *'can_play_with'* is not at all relevant to the word 'Ship', that's why the score is 0.0.\n\nInteresting thing to note here is that the words 'Cat' and 'Dog' have very similar scores for all the features. This makes them very similar words and in reality, they do have many common characteristics too. \n\nIf we consider a 6D vector space (also called *semantic space*) based on these features, then obviously, 'Cat' and 'Dog' are going to be very close to each other over there. However, the words 'Chocolate' and 'Ship' will be far away.\n\nThis is how, by using vectors, we can mathematically express the meaning of words to a computer. These vectors are known as *embedding* vectors.\n\nAlthough embedding vectors are very helpful, they are very difficult to design. We do not really know how to design the features.\n\n## Learning word embeddings\n\nNeural networks, as versatile tools, come here to solve the problem. Here, we will learn an algorithm, called [**word2vec**](https://arxiv.org/abs/1301.3781) that helps us in getting embedding vectors by using a shallow (not very deep) neural network trained on some texts.\n\n:::{.callout-tip}\nBefore delving into the algorithm, we must understand one simple yet very useful application of one-hot vector and matrix multiplication.\n\nSuppose, we have an embedding matrix (made of stacking the embedding vectors together) $\\mathbf{E}^T_{4 \\times 6}$ just like the above figure. \n\nIf we consider our vocabulary as `['Cat', 'Dog', 'Chocolate', 'Ship']`, then one-hot vector for the word 'Dog' will be $\\vec{v}_{Dog} = \\left(0, 1, 0, 0\\right)$.\n\nNow, if we multiply $\\vec{v}_{Dog}$ and $\\mathbf{E}^T$, then the result of $\\vec{v}_{Dog} \\cdot \\mathbf{E}^T$ gives us the second row of $\\mathbf{E}^T$ which is nothing but the embedding vector of the word 'Dog'.\n\nTherefore, we can use an on-hot vector and extract the corresponding embedding vector from the embedding matrix. This is why embedding matrices are also called *lookup tables*.\n:::\n\nIf we consider a word in a sentence, then it is very obvious to note that the surrounding words have some relationships with it. \n\nAs an example, if we have a sentence like '*the dog is swimming in the pond*', then the words *swimming* and *pond* are related. \n\nThe word *swimming* is the context here, if we know this, we can anticipate other words similar to *pond* as well just by following the context.\n\n### Continuous bag of words (CBOW) model\n\nConsider an incompelte sentence: *the birds are _____ in the sky*. By looking at the words *the*, *birds*, *are*, *on*, *the*, *sky*, can we imagine the missing word? Ofcourse, it must be either **flying** or very similar to it.\n\nThis is exactly what happens when we use the CBOW approach. CBOW looks at the context and tries to figure out the target word as shown below,\n\n![Figure 2: Predicting target word based on context](context-window-cbow.svg){ width=60% }\n\nIn reality, this context window moves towards the right till the time we have covered all possible contexts and targets starting from the beginning of our text data to the end. Don't worry, the code example, given below, will make this easier to understand. \n\nWe will now consider a sample text and prepare the training data using CBOW approach for a shallow feed-forward network. It's going to have a single hidden layer. The number of neurons in the hidden layer is controlled by the dimension of the embedding vector.\n\nA pseudo network architecture is shown below,\n\n![Figure 3: A pseudo CBOW network](shallow-network-cbow.svg){ width=60% }\n\nIn this network, each word in the context is fed to the network as an one-hot vector. Having the input vector, the network does some computations to form the hidden layer and eventually tries to predict the target word. The source code depicting the network architecture is given below.\n\n::: {#6df78607 .cell execution_count=4}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Word2Vec(nn.Module):\n    def __init__(self, vocab_size, embedding_dim):\n        super(Word2Vec, self).__init__()\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        \n        # define weight matrices\n        self.weight_i2h = nn.Parameter(torch.randn(\n            (self.embedding_dim, self.vocab_size), \n            dtype=torch.float\n        ))\n        self.weight_h2o = nn.Parameter(torch.randn(\n            (self.vocab_size, self.embedding_dim), \n            dtype=torch.float\n        ))\n\n        self.params = nn.ParameterList(\n            [\n                self.weight_i2h, \n                self.weight_h2o \n            ]\n        )\n\n    def forward(self, input):\n        # create onehot vector of the input\n        input_onehot = F.one_hot(input, self.vocab_size)\n\n        # compute hidden layer\n        hidden = F.relu(\n            torch.matmul(\n                self.weight_i2h, \n                input_onehot.view(self.vocab_size, -1).float()\n            )\n        )\n\n        # compute output\n        output = torch.matmul(\n            self.weight_h2o,\n            hidden\n        )\n\n        # compute log softmax\n        log_probs = F.log_softmax(output.view(1, -1), dim=1)\n\n        return log_probs\n```\n:::\n\n\nNote the output dimension of the network. It is exactly equal to the size of the vocabulary as there are that many number of possibilities while predicting the target word.\n\nSo, essentially, this whole process is a multi-class classification process. \n\nIt is strange to know that prediction is not what we actually focus on here. We are interested in the weight matrix $\\mathbf{E}$. This is our embedding matrix filled with optimally trained parameters. \n\n$\\mathbf{E}$ has the dimension $dim(\\text{embedding vector}) \\times dim(\\text{one-hot vector})$.\n\nHence, $\\vec{v}_{word_j} \\cdot \\mathbf{E}^T$ just gives us the embedding vector for $word_j$.\n\nLet us now see how the training data is prepared for the CBOW network from a sample text.\n\n::: {#afbf6ed6 .cell execution_count=5}\n``` {.python .cell-code}\n# imports\nimport re\nfrom pprint import pprint\n\nraw_text = \"\"\"We are about to study the idea of a computational process.\nComputational processes are abstract beings that inhabit computers.\nAs they evolve, processes manipulate other abstract things called data.\nThe evolution of a process is directed by a pattern of rules\ncalled a program. People create programs to direct processes. In effect,\nwe conjure the spirits of the computer with our spells.\"\"\"\n\n# clean raw text\ncleaned_text = re.sub(r\"[\\.\\,\\-]\", \" \", raw_text).lower().replace('\\n',\"\")\ndata = cleaned_text.lower().split();\n\n# source: https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n```\n:::\n\n\nWe'll now prepare the training data using the code below,\n\n::: {#ee4dc1e7 .cell execution_count=6}\n``` {.python .cell-code}\n# specify context size and embedding dimension\nCONTEXT_SIZE = 2 # two words on the left and two words on the right\nEMBEDDING_DIM = 10\n\n# create vocabulary\nsorted_vocab = sorted(list(set(data)))\n\n# prepare dataset for cbow network\ncbow_data = []\n\n# define start and end indices\nstart_ix = CONTEXT_SIZE; end_ix = len(data) - CONTEXT_SIZE\n\nfor i in range(start_ix, end_ix):\n    target = data[i]\n    for j in range((i - CONTEXT_SIZE), (i + CONTEXT_SIZE + 1)):\n        if j != i:\n            cbow_data.append((data[j], target))\n```\n:::\n\n\nand here is the partial training data as the output,\n\n::: {#904b7dd5 .cell execution_count=7}\n``` {.python .cell-code}\npprint(cbow_data[:10])\n\n[('we', 'about'),\n ('are', 'about'),\n ('to', 'about'),\n ('study', 'about'),\n ('are', 'to'),\n ('about', 'to'),\n ('study', 'to'),\n ('the', 'to'),\n ('about', 'study'),\n ('to', 'study')]\n```\n:::\n\n\n### Skip-gram model\n\nSkip gram, being another variation of word2vec, does the exact opposite of what CBOW does.\n\nIt tries to predict the context or surrounding words by looking at a specific word in a sentence as shown below,\n\n![Figure 4: Predicting context based on target word](context-window-skip-gram.svg){ width=60% }\n\nHere is the code example that helps to prepare the training data based on the same raw text,\n\n::: {#8cf89ff3 .cell execution_count=8}\n``` {.python .cell-code}\n# prepare dataset for cbow network\nskip_gram_data = []\n\n# define start and end indices\nstart_ix = CONTEXT_SIZE; end_ix = len(data) - CONTEXT_SIZE\n\nfor i in range(start_ix, end_ix):\n    target = data[i]\n    for j in range((i - CONTEXT_SIZE), (i + CONTEXT_SIZE + 1)):\n        if j != i:\n            skip_gram_data.append((target, data[j]))\n```\n:::\n\n\nand below is the partial training data as output,\n\n::: {#72f34aa4 .cell execution_count=9}\n``` {.python .cell-code}\npprint(skip_gram_data[:10])\n\n[('about', 'we'),\n ('about', 'are'),\n ('about', 'to'),\n ('about', 'study'),\n ('to', 'are'),\n ('to', 'about'),\n ('to', 'study'),\n ('to', 'the'),\n ('study', 'about'),\n ('study', 'to')]\n```\n:::\n\n\nCBOW and Skip-Gram training data look very similar, the only difference is in the usage of target and context words in the training data. \n\nThe source code for training such a network is given in this [colab notebook](https://colab.research.google.com/drive/1PJCyPULfcYcsUpkvPeQsu4GNsp5m3b3v?usp=sharing). It may not be optimised, but it will give you enough confidence to understand the whole story behind word2vec.\n\n## Pre-trained embedding models\n\nTo get very accurate vector representations of words, we need to train such networks on very large corpus with a much larger embedding dimension. \n\nResearchers have come up with pre-trained embedding models which are trained on massive amounts of text such as the wikipedia. These models are typically of 300 to 600 dimensional and are ready to be consumed.\n\nThe only problem with pre-trained models is the training data is very generic. In case someone needs to work on a specific domain, then such a network should be trained on domain specific corpus.\n\n# Conclusion\n\nSo we are now at the end of the story. Thanks for having some time reading this article. I hope you have enjoyed it and most probably have got the idea and intuition behind the algorithm as well.\n\nThe *word2vec* is a decade old method now, but I believe it has inspired many other developments which have drastically improved the quality of embeddings. \n\nI do wish to research more on the advanced topics in this regard and bring them to you in the form of blog posts in near future. \n\nIf you have any comments on this post, please feel free to leave them below. Thank you.\n\n# Reference\n- [Efficient Estimation of Word Representations in Vector Space by Tomas Mikolov et al.](https://arxiv.org/abs/1301.3781)\n- [Colab notebook to play with the code](https://colab.research.google.com/drive/1PJCyPULfcYcsUpkvPeQsu4GNsp5m3b3v?usp=sharing)\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}