<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Koushik Khan">
<meta name="dcterms.date" content="2023-05-01">
<meta name="description" content="Representing texts in a better way that neural networks understand">

<title>Make More Sense of Language Through Embeddings â€“ Koushik Khan</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../resources/img/favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Koushik Khan</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../projects.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../skills.html"> 
<span class="menu-text">Skills</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../resume.html"> 
<span class="menu-text">Resume</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="mailto:koushik.khan.ds@gmail.com"> 
<span class="menu-text"><i class="fa-solid fa-envelope fa-large" aria-label="envelope"></i></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://linkedin.com/in/koushikkhan/"> 
<span class="menu-text"><i class="fa-brands fa-linkedin-in fa-large" aria-label="linkedin-in"></i></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://x.com/koushikkhan92/"> 
<span class="menu-text"><i class="fa-brands fa-x-twitter fa-large" aria-label="x-twitter"></i></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://www.github.com/koushikkhan/"> 
<span class="menu-text"><i class="fa-brands fa-github fa-large" aria-label="github"></i></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Make More Sense of Language Through Embeddings</h1>
                  <div>
        <div class="description">
          Representing texts in a better way that neural networks understand
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">deep learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Koushik Khan </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 1, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#some-important-terminologies" id="toc-some-important-terminologies" class="nav-link" data-scroll-target="#some-important-terminologies">Some important terminologies</a></li>
  <li><a href="#converting-texts-to-numbers" id="toc-converting-texts-to-numbers" class="nav-link" data-scroll-target="#converting-texts-to-numbers">Converting texts to numbers</a></li>
  <li><a href="#better-ways-to-represent-texts" id="toc-better-ways-to-represent-texts" class="nav-link" data-scroll-target="#better-ways-to-represent-texts">Better ways to represent texts</a>
  <ul class="collapse">
  <li><a href="#one-hot-vector" id="toc-one-hot-vector" class="nav-link" data-scroll-target="#one-hot-vector">One-hot vector</a></li>
  <li><a href="#embedding-vector" id="toc-embedding-vector" class="nav-link" data-scroll-target="#embedding-vector">Embedding vector</a></li>
  <li><a href="#learning-word-embeddings" id="toc-learning-word-embeddings" class="nav-link" data-scroll-target="#learning-word-embeddings">Learning word embeddings</a>
  <ul class="collapse">
  <li><a href="#continuous-bag-of-words-cbow-model" id="toc-continuous-bag-of-words-cbow-model" class="nav-link" data-scroll-target="#continuous-bag-of-words-cbow-model">Continuous bag of words (CBOW) model</a></li>
  <li><a href="#skip-gram-model" id="toc-skip-gram-model" class="nav-link" data-scroll-target="#skip-gram-model">Skip-gram model</a></li>
  </ul></li>
  <li><a href="#pre-trained-embedding-models" id="toc-pre-trained-embedding-models" class="nav-link" data-scroll-target="#pre-trained-embedding-models">Pre-trained embedding models</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#reference" id="toc-reference" class="nav-link" data-scroll-target="#reference">Reference</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<center>
<img src="text.jpg" alt="alt text" title="dfg" height="400" width="800">
<figcaption>
Photo by <a href="https://unsplash.com/@melpoole?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Mel Poole</a> on <a href="https://unsplash.com/photos/lBsvzgYnzPU?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a>
</figcaption>
</center>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>A computer algorithm is nothing but a sequence of carefully chosen steps having a specific goal, it needs some input data to work on and produce some results. Achieving this goal depends on many many factors and representing the input data to the algorithm is one of them.</p>
<p>Neural networks, being some complex algorithms, are capable of performing many useful tasks with human languages when represented both as texts and sound waves. Some useful examples can be <em>human sentiment detection</em> from written text, <em>language translation</em>, <em>image captioning</em> etc.</p>
<p>In all these cases, one interesting thing is to know how such a massive amount of textual data are supplied to a network so that it understands the inner meaning just like a human.</p>
<p>In this post I will focus on a couple of methods to represent a textual dataset to a network, collectively known as word2vec.</p>
</section>
<section id="some-important-terminologies" class="level1">
<h1>Some important terminologies</h1>
<p>Let us understand some basic yet important terminologies that are used while working with textual data. For the sake of convenience, we will consider <em>English</em> as the language to understand various concepts in this post, but the same terminologies are applicable for other languages too.</p>
<ul>
<li><p><strong>Corpus</strong>: A corpus is a collection of textual data. It typically refers to a group of sentences or paragraphs. For example, the messages we send to someone in any messaging platform can be considered together to form a corpus.</p></li>
<li><p><strong>Token</strong>: A token is the most granular part of a corpus. Depending on the requirement, it can be a single character, a single word or even a single sentence. The process of creating tokens out of a corpus is called <em>tokenization</em>.</p></li>
<li><p><strong>Vocabulary</strong>: A vocabulary is the set of unique (non-repetitive) tokens. Technically, it is that set which generates the entire corpus.</p></li>
</ul>
<p>An example will help here to understand it better.</p>
<div id="54c4d1c6" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a>corpus <span class="op">=</span> [<span class="st">'Mathematics is the language to understand the nature'</span>]</span>
<span id="cb1-2"><a href="#cb1-2"></a></span>
<span id="cb1-3"><a href="#cb1-3"></a>word_level_tokens <span class="op">=</span> [</span>
<span id="cb1-4"><a href="#cb1-4"></a>  <span class="st">'Mathematics'</span>, <span class="st">'is'</span>, <span class="st">'the'</span>, </span>
<span id="cb1-5"><a href="#cb1-5"></a>  <span class="st">'language'</span>, <span class="st">'to'</span>, <span class="st">'understand'</span>, </span>
<span id="cb1-6"><a href="#cb1-6"></a>  <span class="st">'the'</span>, <span class="st">'nature'</span></span>
<span id="cb1-7"><a href="#cb1-7"></a>]</span>
<span id="cb1-8"><a href="#cb1-8"></a></span>
<span id="cb1-9"><a href="#cb1-9"></a>vocabulary <span class="op">=</span> {<span class="st">'Mathematics'</span>, <span class="st">'is'</span>, <span class="st">'the'</span>, <span class="st">'language'</span>, <span class="st">'to'</span>, <span class="st">'understand'</span>, <span class="st">'nature'</span>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that, the word <code>the</code> has appeared twice in the corpus and it is considered only once in the vocabulary.</p>
</section>
<section id="converting-texts-to-numbers" class="level1">
<h1>Converting texts to numbers</h1>
<p>Converting texts to numbers are required because a computer can only handle numbers, although typically not in the form we give. It converts numbers (decimal numbers) to the binary form which is basically a sequence of zeros and ones, before performing any task.</p>
<p>Since the beginning of development of modern computers, we had something called <a href="https://www.ascii-code.com/">ASCII table</a>. This table has numeric representation of each character in English including numbers and some special characters.</p>
</section>
<section id="better-ways-to-represent-texts" class="level1">
<h1>Better ways to represent texts</h1>
<p>ASCII codes are certainly useful to represent characters in the computersâ€™ memory. But it does not fulfill the purpose while developing modern intelligent applications like a language translator.</p>
<p>In natural language processing, tokens are usually represented by numerical arrays or vectors. Let us understand the various ways to create such vectors.</p>
<p>Before proceeding further, let us consider a new corpus (a group of sentences quoted by <a href="https://en.wikipedia.org/wiki/Shakuntala_Devi">Shakuntala Devi</a>) with a word level tokenization as below:</p>
<div id="cd5fd48e" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a>corpus <span class="op">=</span> [</span>
<span id="cb2-2"><a href="#cb2-2"></a>  <span class="st">'without mathematics, thereâ€™s nothing you can do'</span>,</span>
<span id="cb2-3"><a href="#cb2-3"></a>  <span class="st">'everything around you is mathematics'</span>,</span>
<span id="cb2-4"><a href="#cb2-4"></a>  <span class="st">'everything around you is number'</span></span>
<span id="cb2-5"><a href="#cb2-5"></a>]</span>
<span id="cb2-6"><a href="#cb2-6"></a></span>
<span id="cb2-7"><a href="#cb2-7"></a>word_level_tokens <span class="op">=</span> [</span>
<span id="cb2-8"><a href="#cb2-8"></a>  <span class="st">"without"</span>, <span class="st">"mathematics"</span>, <span class="st">"there's"</span>,</span>
<span id="cb2-9"><a href="#cb2-9"></a>  <span class="st">"nothing"</span>, <span class="st">"you"</span>, <span class="st">"can"</span>, <span class="st">"do"</span>, </span>
<span id="cb2-10"><a href="#cb2-10"></a>  <span class="st">"everything"</span>, <span class="st">"around"</span>, <span class="st">"you"</span>, <span class="st">"is"</span>, </span>
<span id="cb2-11"><a href="#cb2-11"></a>  <span class="st">"mathematics"</span>, <span class="st">"everything"</span>, <span class="st">"around"</span>, </span>
<span id="cb2-12"><a href="#cb2-12"></a>  <span class="st">"you"</span>, <span class="st">"is"</span>, <span class="st">"number"</span></span>
<span id="cb2-13"><a href="#cb2-13"></a>]</span>
<span id="cb2-14"><a href="#cb2-14"></a></span>
<span id="cb2-15"><a href="#cb2-15"></a>sorted_vocabulary <span class="op">=</span> [</span>
<span id="cb2-16"><a href="#cb2-16"></a>  <span class="st">'around'</span>,</span>
<span id="cb2-17"><a href="#cb2-17"></a>  <span class="st">'can'</span>,</span>
<span id="cb2-18"><a href="#cb2-18"></a>  <span class="st">'do'</span>,</span>
<span id="cb2-19"><a href="#cb2-19"></a>  <span class="st">'everything'</span>,</span>
<span id="cb2-20"><a href="#cb2-20"></a>  <span class="st">'is'</span>,</span>
<span id="cb2-21"><a href="#cb2-21"></a>  <span class="st">'mathematics'</span>,</span>
<span id="cb2-22"><a href="#cb2-22"></a>  <span class="st">'nothing'</span>,</span>
<span id="cb2-23"><a href="#cb2-23"></a>  <span class="st">'number'</span>,</span>
<span id="cb2-24"><a href="#cb2-24"></a>  <span class="st">"there's"</span>,</span>
<span id="cb2-25"><a href="#cb2-25"></a>  <span class="st">'without'</span>,</span>
<span id="cb2-26"><a href="#cb2-26"></a>  <span class="st">'you'</span></span>
<span id="cb2-27"><a href="#cb2-27"></a>]</span>
<span id="cb2-28"><a href="#cb2-28"></a></span>
<span id="cb2-29"><a href="#cb2-29"></a>vocabulary_size <span class="op">=</span> <span class="dv">11</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="one-hot-vector" class="level2">
<h2 class="anchored" data-anchor-id="one-hot-vector">One-hot vector</h2>
<p>This is the simplest way to convert tokens into a vector. One-hot vector, for a token, is a vector of dimension <code>vocabulary_size</code> (often denoted by <span class="math inline">|V|</span>). In this vector, all the elements are <span class="math inline">0</span>â€™s except a single <span class="math inline">1</span>. The index that holds the <span class="math inline">1</span> is called <em>hot</em> index and it is basically the index of the token in the vocabulary.</p>
<p>Example:</p>
<div id="59d70994" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a>one_hot_for_around <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb3-2"><a href="#cb3-2"></a>one_hot_for_can <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb3-3"><a href="#cb3-3"></a>one_hot_for_number <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As you can imagine, for a large corpus, one-hot vectors are very large dimensional sparse vectors.</p>
<p>Also, if we represent any text using one-hot vectors (<em>technically a matrix made of one-hot vectors</em>), then basically we are not considering word-word relationship or similarity.</p>
<p>Words (rather tokens) are considered independent chunks of the text for this kind of setup, but it is something that we do not like. Any language maintains some contexts through word-word relationships and those are completely ignored by one-hot representation.</p>
<p>If we try to measure the similarity between two words with the help of one-hot representation, we will always get zero, but that does not mean the words are very close on the semantic space.</p>
</section>
<section id="embedding-vector" class="level2">
<h2 class="anchored" data-anchor-id="embedding-vector">Embedding vector</h2>
<p>As you have observed, one-hot vectors do not consider context, so we definitely need something meaningful for representing the tokens mathematically.</p>
<p>In the world of NLP, the term <em>embedding</em> usually refers to a dense vector (a vector that has most of its components as non-zero real numbers) that represents a token. As you know, depending on the problem, a token can be a character, a word or even a full sentence and an embedding vector is created to represent it accordingly.</p>
<p>Let us try to understand the way of expressing the meaning of words mathematically.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="word-features.svg" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>Figure 1: Word features representation</figcaption>
</figure>
</div>
<p>In the above table, we have four different words which are â€˜Chocolateâ€™, â€˜Catâ€™, â€˜Dogâ€™ and â€˜Shipâ€™. For each of these words we have six randomly chosen features or characteristics.</p>
<p>Each of the features can have values in between zero and one. If the value for a feature is very close to zero, it indicates that the feature is not very applicable to the word. Similarly, having a value closer to one indicates that the feature is quite applicable to the word.</p>
<p>As an example, the feature <em>â€˜can_play_withâ€™</em> is not at all relevant to the word â€˜Shipâ€™, thatâ€™s why the score is 0.0.</p>
<p>Interesting thing to note here is that the words â€˜Catâ€™ and â€˜Dogâ€™ have very similar scores for all the features. This makes them very similar words and in reality, they do have many common characteristics too.</p>
<p>If we consider a 6D vector space (also called <em>semantic space</em>) based on these features, then obviously, â€˜Catâ€™ and â€˜Dogâ€™ are going to be very close to each other over there. However, the words â€˜Chocolateâ€™ and â€˜Shipâ€™ will be far away.</p>
<p>This is how, by using vectors, we can mathematically express the meaning of words to a computer. These vectors are known as <em>embedding</em> vectors.</p>
<p>Although embedding vectors are very helpful, they are very difficult to design. We do not really know how to design the features.</p>
</section>
<section id="learning-word-embeddings" class="level2">
<h2 class="anchored" data-anchor-id="learning-word-embeddings">Learning word embeddings</h2>
<p>Neural networks, as versatile tools, come here to solve the problem. Here, we will learn an algorithm, called <a href="https://arxiv.org/abs/1301.3781"><strong>word2vec</strong></a> that helps us in getting embedding vectors by using a shallow (not very deep) neural network trained on some texts.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before delving into the algorithm, we must understand one simple yet very useful application of one-hot vector and matrix multiplication.</p>
<p>Suppose, we have an embedding matrix (made of stacking the embedding vectors together) <span class="math inline">\mathbf{E}^T_{4 \times 6}</span> just like the above figure.</p>
<p>If we consider our vocabulary as <code>['Cat', 'Dog', 'Chocolate', 'Ship']</code>, then one-hot vector for the word â€˜Dogâ€™ will be <span class="math inline">\vec{v}_{Dog} = \left(0, 1, 0, 0\right)</span>.</p>
<p>Now, if we multiply <span class="math inline">\vec{v}_{Dog}</span> and <span class="math inline">\mathbf{E}^T</span>, then the result of <span class="math inline">\vec{v}_{Dog} \cdot \mathbf{E}^T</span> gives us the second row of <span class="math inline">\mathbf{E}^T</span> which is nothing but the embedding vector of the word â€˜Dogâ€™.</p>
<p>Therefore, we can use an on-hot vector and extract the corresponding embedding vector from the embedding matrix. This is why embedding matrices are also called <em>lookup tables</em>.</p>
</div>
</div>
<p>If we consider a word in a sentence, then it is very obvious to note that the surrounding words have some relationships with it.</p>
<p>As an example, if we have a sentence like â€˜<em>the dog is swimming in the pond</em>â€™, then the words <em>swimming</em> and <em>pond</em> are related.</p>
<p>The word <em>swimming</em> is the context here, if we know this, we can anticipate other words similar to <em>pond</em> as well just by following the context.</p>
<section id="continuous-bag-of-words-cbow-model" class="level3">
<h3 class="anchored" data-anchor-id="continuous-bag-of-words-cbow-model">Continuous bag of words (CBOW) model</h3>
<p>Consider an incompelte sentence: <em>the birds are _____ in the sky</em>. By looking at the words <em>the</em>, <em>birds</em>, <em>are</em>, <em>on</em>, <em>the</em>, <em>sky</em>, can we imagine the missing word? Ofcourse, it must be either <strong>flying</strong> or very similar to it.</p>
<p>This is exactly what happens when we use the CBOW approach. CBOW looks at the context and tries to figure out the target word as shown below,</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="context-window-cbow.svg" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>Figure 2: Predicting target word based on context</figcaption>
</figure>
</div>
<p>In reality, this context window moves towards the right till the time we have covered all possible contexts and targets starting from the beginning of our text data to the end. Donâ€™t worry, the code example, given below, will make this easier to understand.</p>
<p>We will now consider a sample text and prepare the training data using CBOW approach for a shallow feed-forward network. Itâ€™s going to have a single hidden layer. The number of neurons in the hidden layer is controlled by the dimension of the embedding vector.</p>
<p>A pseudo network architecture is shown below,</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="shallow-network-cbow.svg" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>Figure 3: A pseudo CBOW network</figcaption>
</figure>
</div>
<p>In this network, each word in the context is fed to the network as an one-hot vector. Having the input vector, the network does some computations to form the hidden layer and eventually tries to predict the target word. The source code depicting the network architecture is given below.</p>
<div id="6df78607" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="im">import</span> torch</span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb4-3"><a href="#cb4-3"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb4-4"><a href="#cb4-4"></a></span>
<span id="cb4-5"><a href="#cb4-5"></a><span class="kw">class</span> Word2Vec(nn.Module):</span>
<span id="cb4-6"><a href="#cb4-6"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, embedding_dim):</span>
<span id="cb4-7"><a href="#cb4-7"></a>        <span class="bu">super</span>(Word2Vec, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb4-8"><a href="#cb4-8"></a>        <span class="va">self</span>.vocab_size <span class="op">=</span> vocab_size</span>
<span id="cb4-9"><a href="#cb4-9"></a>        <span class="va">self</span>.embedding_dim <span class="op">=</span> embedding_dim</span>
<span id="cb4-10"><a href="#cb4-10"></a>        </span>
<span id="cb4-11"><a href="#cb4-11"></a>        <span class="co"># define weight matrices</span></span>
<span id="cb4-12"><a href="#cb4-12"></a>        <span class="va">self</span>.weight_i2h <span class="op">=</span> nn.Parameter(torch.randn(</span>
<span id="cb4-13"><a href="#cb4-13"></a>            (<span class="va">self</span>.embedding_dim, <span class="va">self</span>.vocab_size), </span>
<span id="cb4-14"><a href="#cb4-14"></a>            dtype<span class="op">=</span>torch.<span class="bu">float</span></span>
<span id="cb4-15"><a href="#cb4-15"></a>        ))</span>
<span id="cb4-16"><a href="#cb4-16"></a>        <span class="va">self</span>.weight_h2o <span class="op">=</span> nn.Parameter(torch.randn(</span>
<span id="cb4-17"><a href="#cb4-17"></a>            (<span class="va">self</span>.vocab_size, <span class="va">self</span>.embedding_dim), </span>
<span id="cb4-18"><a href="#cb4-18"></a>            dtype<span class="op">=</span>torch.<span class="bu">float</span></span>
<span id="cb4-19"><a href="#cb4-19"></a>        ))</span>
<span id="cb4-20"><a href="#cb4-20"></a></span>
<span id="cb4-21"><a href="#cb4-21"></a>        <span class="va">self</span>.params <span class="op">=</span> nn.ParameterList(</span>
<span id="cb4-22"><a href="#cb4-22"></a>            [</span>
<span id="cb4-23"><a href="#cb4-23"></a>                <span class="va">self</span>.weight_i2h, </span>
<span id="cb4-24"><a href="#cb4-24"></a>                <span class="va">self</span>.weight_h2o </span>
<span id="cb4-25"><a href="#cb4-25"></a>            ]</span>
<span id="cb4-26"><a href="#cb4-26"></a>        )</span>
<span id="cb4-27"><a href="#cb4-27"></a></span>
<span id="cb4-28"><a href="#cb4-28"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb4-29"><a href="#cb4-29"></a>        <span class="co"># create onehot vector of the input</span></span>
<span id="cb4-30"><a href="#cb4-30"></a>        input_onehot <span class="op">=</span> F.one_hot(<span class="bu">input</span>, <span class="va">self</span>.vocab_size)</span>
<span id="cb4-31"><a href="#cb4-31"></a></span>
<span id="cb4-32"><a href="#cb4-32"></a>        <span class="co"># compute hidden layer</span></span>
<span id="cb4-33"><a href="#cb4-33"></a>        hidden <span class="op">=</span> F.relu(</span>
<span id="cb4-34"><a href="#cb4-34"></a>            torch.matmul(</span>
<span id="cb4-35"><a href="#cb4-35"></a>                <span class="va">self</span>.weight_i2h, </span>
<span id="cb4-36"><a href="#cb4-36"></a>                input_onehot.view(<span class="va">self</span>.vocab_size, <span class="op">-</span><span class="dv">1</span>).<span class="bu">float</span>()</span>
<span id="cb4-37"><a href="#cb4-37"></a>            )</span>
<span id="cb4-38"><a href="#cb4-38"></a>        )</span>
<span id="cb4-39"><a href="#cb4-39"></a></span>
<span id="cb4-40"><a href="#cb4-40"></a>        <span class="co"># compute output</span></span>
<span id="cb4-41"><a href="#cb4-41"></a>        output <span class="op">=</span> torch.matmul(</span>
<span id="cb4-42"><a href="#cb4-42"></a>            <span class="va">self</span>.weight_h2o,</span>
<span id="cb4-43"><a href="#cb4-43"></a>            hidden</span>
<span id="cb4-44"><a href="#cb4-44"></a>        )</span>
<span id="cb4-45"><a href="#cb4-45"></a></span>
<span id="cb4-46"><a href="#cb4-46"></a>        <span class="co"># compute log softmax</span></span>
<span id="cb4-47"><a href="#cb4-47"></a>        log_probs <span class="op">=</span> F.log_softmax(output.view(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-48"><a href="#cb4-48"></a></span>
<span id="cb4-49"><a href="#cb4-49"></a>        <span class="cf">return</span> log_probs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note the output dimension of the network. It is exactly equal to the size of the vocabulary as there are that many number of possibilities while predicting the target word.</p>
<p>So, essentially, this whole process is a multi-class classification process.</p>
<p>It is strange to know that prediction is not what we actually focus on here. We are interested in the weight matrix <span class="math inline">\mathbf{E}</span>. This is our embedding matrix filled with optimally trained parameters.</p>
<p><span class="math inline">\mathbf{E}</span> has the dimension <span class="math inline">dim(\text{embedding vector}) \times dim(\text{one-hot vector})</span>.</p>
<p>Hence, <span class="math inline">\vec{v}_{word_j} \cdot \mathbf{E}^T</span> just gives us the embedding vector for <span class="math inline">word_j</span>.</p>
<p>Let us now see how the training data is prepared for the CBOW network from a sample text.</p>
<div id="afbf6ed6" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="co"># imports</span></span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="im">import</span> re</span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="im">from</span> pprint <span class="im">import</span> pprint</span>
<span id="cb5-4"><a href="#cb5-4"></a></span>
<span id="cb5-5"><a href="#cb5-5"></a>raw_text <span class="op">=</span> <span class="st">"""We are about to study the idea of a computational process.</span></span>
<span id="cb5-6"><a href="#cb5-6"></a><span class="st">Computational processes are abstract beings that inhabit computers.</span></span>
<span id="cb5-7"><a href="#cb5-7"></a><span class="st">As they evolve, processes manipulate other abstract things called data.</span></span>
<span id="cb5-8"><a href="#cb5-8"></a><span class="st">The evolution of a process is directed by a pattern of rules</span></span>
<span id="cb5-9"><a href="#cb5-9"></a><span class="st">called a program. People create programs to direct processes. In effect,</span></span>
<span id="cb5-10"><a href="#cb5-10"></a><span class="st">we conjure the spirits of the computer with our spells."""</span></span>
<span id="cb5-11"><a href="#cb5-11"></a></span>
<span id="cb5-12"><a href="#cb5-12"></a><span class="co"># clean raw text</span></span>
<span id="cb5-13"><a href="#cb5-13"></a>cleaned_text <span class="op">=</span> re.sub(<span class="vs">r"[\.\,\-]"</span>, <span class="st">" "</span>, raw_text).lower().replace(<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>,<span class="st">""</span>)</span>
<span id="cb5-14"><a href="#cb5-14"></a>data <span class="op">=</span> cleaned_text.lower().split()<span class="op">;</span></span>
<span id="cb5-15"><a href="#cb5-15"></a></span>
<span id="cb5-16"><a href="#cb5-16"></a><span class="co"># source: https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Weâ€™ll now prepare the training data using the code below,</p>
<div id="ee4dc1e7" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="co"># specify context size and embedding dimension</span></span>
<span id="cb6-2"><a href="#cb6-2"></a>CONTEXT_SIZE <span class="op">=</span> <span class="dv">2</span> <span class="co"># two words on the left and two words on the right</span></span>
<span id="cb6-3"><a href="#cb6-3"></a>EMBEDDING_DIM <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb6-4"><a href="#cb6-4"></a></span>
<span id="cb6-5"><a href="#cb6-5"></a><span class="co"># create vocabulary</span></span>
<span id="cb6-6"><a href="#cb6-6"></a>sorted_vocab <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(data)))</span>
<span id="cb6-7"><a href="#cb6-7"></a></span>
<span id="cb6-8"><a href="#cb6-8"></a><span class="co"># prepare dataset for cbow network</span></span>
<span id="cb6-9"><a href="#cb6-9"></a>cbow_data <span class="op">=</span> []</span>
<span id="cb6-10"><a href="#cb6-10"></a></span>
<span id="cb6-11"><a href="#cb6-11"></a><span class="co"># define start and end indices</span></span>
<span id="cb6-12"><a href="#cb6-12"></a>start_ix <span class="op">=</span> CONTEXT_SIZE<span class="op">;</span> end_ix <span class="op">=</span> <span class="bu">len</span>(data) <span class="op">-</span> CONTEXT_SIZE</span>
<span id="cb6-13"><a href="#cb6-13"></a></span>
<span id="cb6-14"><a href="#cb6-14"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(start_ix, end_ix):</span>
<span id="cb6-15"><a href="#cb6-15"></a>    target <span class="op">=</span> data[i]</span>
<span id="cb6-16"><a href="#cb6-16"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>((i <span class="op">-</span> CONTEXT_SIZE), (i <span class="op">+</span> CONTEXT_SIZE <span class="op">+</span> <span class="dv">1</span>)):</span>
<span id="cb6-17"><a href="#cb6-17"></a>        <span class="cf">if</span> j <span class="op">!=</span> i:</span>
<span id="cb6-18"><a href="#cb6-18"></a>            cbow_data.append((data[j], target))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>and here is the partial training data as the output,</p>
<div id="904b7dd5" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a>pprint(cbow_data[:<span class="dv">10</span>])</span>
<span id="cb7-2"><a href="#cb7-2"></a></span>
<span id="cb7-3"><a href="#cb7-3"></a>[(<span class="st">'we'</span>, <span class="st">'about'</span>),</span>
<span id="cb7-4"><a href="#cb7-4"></a> (<span class="st">'are'</span>, <span class="st">'about'</span>),</span>
<span id="cb7-5"><a href="#cb7-5"></a> (<span class="st">'to'</span>, <span class="st">'about'</span>),</span>
<span id="cb7-6"><a href="#cb7-6"></a> (<span class="st">'study'</span>, <span class="st">'about'</span>),</span>
<span id="cb7-7"><a href="#cb7-7"></a> (<span class="st">'are'</span>, <span class="st">'to'</span>),</span>
<span id="cb7-8"><a href="#cb7-8"></a> (<span class="st">'about'</span>, <span class="st">'to'</span>),</span>
<span id="cb7-9"><a href="#cb7-9"></a> (<span class="st">'study'</span>, <span class="st">'to'</span>),</span>
<span id="cb7-10"><a href="#cb7-10"></a> (<span class="st">'the'</span>, <span class="st">'to'</span>),</span>
<span id="cb7-11"><a href="#cb7-11"></a> (<span class="st">'about'</span>, <span class="st">'study'</span>),</span>
<span id="cb7-12"><a href="#cb7-12"></a> (<span class="st">'to'</span>, <span class="st">'study'</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="skip-gram-model" class="level3">
<h3 class="anchored" data-anchor-id="skip-gram-model">Skip-gram model</h3>
<p>Skip gram, being another variation of word2vec, does the exact opposite of what CBOW does.</p>
<p>It tries to predict the context or surrounding words by looking at a specific word in a sentence as shown below,</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="context-window-skip-gram.svg" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>Figure 4: Predicting context based on target word</figcaption>
</figure>
</div>
<p>Here is the code example that helps to prepare the training data based on the same raw text,</p>
<div id="8cf89ff3" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="co"># prepare dataset for cbow network</span></span>
<span id="cb8-2"><a href="#cb8-2"></a>skip_gram_data <span class="op">=</span> []</span>
<span id="cb8-3"><a href="#cb8-3"></a></span>
<span id="cb8-4"><a href="#cb8-4"></a><span class="co"># define start and end indices</span></span>
<span id="cb8-5"><a href="#cb8-5"></a>start_ix <span class="op">=</span> CONTEXT_SIZE<span class="op">;</span> end_ix <span class="op">=</span> <span class="bu">len</span>(data) <span class="op">-</span> CONTEXT_SIZE</span>
<span id="cb8-6"><a href="#cb8-6"></a></span>
<span id="cb8-7"><a href="#cb8-7"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(start_ix, end_ix):</span>
<span id="cb8-8"><a href="#cb8-8"></a>    target <span class="op">=</span> data[i]</span>
<span id="cb8-9"><a href="#cb8-9"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>((i <span class="op">-</span> CONTEXT_SIZE), (i <span class="op">+</span> CONTEXT_SIZE <span class="op">+</span> <span class="dv">1</span>)):</span>
<span id="cb8-10"><a href="#cb8-10"></a>        <span class="cf">if</span> j <span class="op">!=</span> i:</span>
<span id="cb8-11"><a href="#cb8-11"></a>            skip_gram_data.append((target, data[j]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>and below is the partial training data as output,</p>
<div id="72f34aa4" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a>pprint(skip_gram_data[:<span class="dv">10</span>])</span>
<span id="cb9-2"><a href="#cb9-2"></a></span>
<span id="cb9-3"><a href="#cb9-3"></a>[(<span class="st">'about'</span>, <span class="st">'we'</span>),</span>
<span id="cb9-4"><a href="#cb9-4"></a> (<span class="st">'about'</span>, <span class="st">'are'</span>),</span>
<span id="cb9-5"><a href="#cb9-5"></a> (<span class="st">'about'</span>, <span class="st">'to'</span>),</span>
<span id="cb9-6"><a href="#cb9-6"></a> (<span class="st">'about'</span>, <span class="st">'study'</span>),</span>
<span id="cb9-7"><a href="#cb9-7"></a> (<span class="st">'to'</span>, <span class="st">'are'</span>),</span>
<span id="cb9-8"><a href="#cb9-8"></a> (<span class="st">'to'</span>, <span class="st">'about'</span>),</span>
<span id="cb9-9"><a href="#cb9-9"></a> (<span class="st">'to'</span>, <span class="st">'study'</span>),</span>
<span id="cb9-10"><a href="#cb9-10"></a> (<span class="st">'to'</span>, <span class="st">'the'</span>),</span>
<span id="cb9-11"><a href="#cb9-11"></a> (<span class="st">'study'</span>, <span class="st">'about'</span>),</span>
<span id="cb9-12"><a href="#cb9-12"></a> (<span class="st">'study'</span>, <span class="st">'to'</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>CBOW and Skip-Gram training data look very similar, the only difference is in the usage of target and context words in the training data.</p>
<p>The source code for training such a network is given in this <a href="https://colab.research.google.com/drive/1PJCyPULfcYcsUpkvPeQsu4GNsp5m3b3v?usp=sharing">colab notebook</a>. It may not be optimised, but it will give you enough confidence to understand the whole story behind word2vec.</p>
</section>
</section>
<section id="pre-trained-embedding-models" class="level2">
<h2 class="anchored" data-anchor-id="pre-trained-embedding-models">Pre-trained embedding models</h2>
<p>To get very accurate vector representations of words, we need to train such networks on very large corpus with a much larger embedding dimension.</p>
<p>Researchers have come up with pre-trained embedding models which are trained on massive amounts of text such as the wikipedia. These models are typically of 300 to 600 dimensional and are ready to be consumed.</p>
<p>The only problem with pre-trained models is the training data is very generic. In case someone needs to work on a specific domain, then such a network should be trained on domain specific corpus.</p>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>So we are now at the end of the story. Thanks for having some time reading this article. I hope you have enjoyed it and most probably have got the idea and intuition behind the algorithm as well.</p>
<p>The <em>word2vec</em> is a decade old method now, but I believe it has inspired many other developments which have drastically improved the quality of embeddings.</p>
<p>I do wish to research more on the advanced topics in this regard and bring them to you in the form of blog posts in near future.</p>
<p>If you have any comments on this post, please feel free to leave them below. Thank you.</p>
</section>
<section id="reference" class="level1">
<h1>Reference</h1>
<ul>
<li><a href="https://arxiv.org/abs/1301.3781">Efficient Estimation of Word Representations in Vector Space by Tomas Mikolov et al.</a></li>
<li><a href="https://colab.research.google.com/drive/1PJCyPULfcYcsUpkvPeQsu4GNsp5m3b3v?usp=sharing">Colab notebook to play with the code</a></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/koushikkhan\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="koushikkhan/blog-discussions" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->




</body></html>