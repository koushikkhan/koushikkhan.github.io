<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Koushik Khan">
<meta name="dcterms.date" content="2023-04-22">
<meta name="description" content="The way, deep neural networks learn">

<title>Gradient Calculation Through Computational Graph – Koushik Khan</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../resources/img/favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-265de45eb6cd83cfc12e0465bebb4b45.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-2e92b42fce8a9d0e27d970132097cb03.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<meta name="mermaid-theme" content="default">
<script src="../../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Koushik Khan</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../tutorials.html"> 
<span class="menu-text">Tutorials</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../projects.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../skills.html"> 
<span class="menu-text">Skills</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../resume.html"> 
<span class="menu-text">Resume</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="mailto:koushik.khan.ds@gmail.com"> 
<span class="menu-text"><i class="fa-solid fa-envelope fa-large" aria-label="envelope"></i></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://linkedin.com/in/koushikkhan/"> 
<span class="menu-text"><i class="fa-brands fa-linkedin-in fa-large" aria-label="linkedin-in"></i></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://x.com/koushikkhan92/"> 
<span class="menu-text"><i class="fa-brands fa-x-twitter fa-large" aria-label="x-twitter"></i></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://www.github.com/koushikkhan/"> 
<span class="menu-text"><i class="fa-brands fa-github fa-large" aria-label="github"></i></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Gradient Calculation Through Computational Graph</h1>
                  <div>
        <div class="description">
          The way, deep neural networks learn
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">deep learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Koushik Khan </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">April 22, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#differentiation" id="toc-differentiation" class="nav-link" data-scroll-target="#differentiation">Differentiation</a>
  <ul class="collapse">
  <li><a href="#derivative-for-different-type-of-input-mappings" id="toc-derivative-for-different-type-of-input-mappings" class="nav-link" data-scroll-target="#derivative-for-different-type-of-input-mappings">Derivative for different type of input mappings</a>
  <ul class="collapse">
  <li><a href="#functions-of-type-f-mathbbrn-to-mathbbr---vector-in-scalar-out" id="toc-functions-of-type-f-mathbbrn-to-mathbbr---vector-in-scalar-out" class="nav-link" data-scroll-target="#functions-of-type-f-mathbbrn-to-mathbbr---vector-in-scalar-out">Functions of type <span class="math inline">f: \mathbb{R}^n \to \mathbb{R}</span> - vector in, scalar out</a></li>
  <li><a href="#functions-of-type-f-mathbbrn-to-mathbbrm---vector-in-vector-out" id="toc-functions-of-type-f-mathbbrn-to-mathbbrm---vector-in-vector-out" class="nav-link" data-scroll-target="#functions-of-type-f-mathbbrn-to-mathbbrm---vector-in-vector-out">Functions of type <span class="math inline">f: \mathbb{R}^n \to \mathbb{R}^m</span> - vector in, vector out</a></li>
  </ul></li>
  <li><a href="#the-chain-rule-of-differentiation" id="toc-the-chain-rule-of-differentiation" class="nav-link" data-scroll-target="#the-chain-rule-of-differentiation">The chain rule of differentiation</a></li>
  <li><a href="#partial-differentiation" id="toc-partial-differentiation" class="nav-link" data-scroll-target="#partial-differentiation">Partial differentiation</a></li>
  </ul></li>
  <li><a href="#the-computational-graph" id="toc-the-computational-graph" class="nav-link" data-scroll-target="#the-computational-graph">The computational graph</a>
  <ul class="collapse">
  <li><a href="#differentiation-through-a-graph" id="toc-differentiation-through-a-graph" class="nav-link" data-scroll-target="#differentiation-through-a-graph">Differentiation through a graph</a></li>
  <li><a href="#important-takeaways" id="toc-important-takeaways" class="nav-link" data-scroll-target="#important-takeaways">Important takeaways</a></li>
  <li><a href="#sorting-nodes-before-performing-backpropagation" id="toc-sorting-nodes-before-performing-backpropagation" class="nav-link" data-scroll-target="#sorting-nodes-before-performing-backpropagation">Sorting nodes before performing backpropagation</a></li>
  <li><a href="#the-efficient-vector-jacobian-product" id="toc-the-efficient-vector-jacobian-product" class="nav-link" data-scroll-target="#the-efficient-vector-jacobian-product">The efficient vector-jacobian product</a></li>
  <li><a href="#the-training-loop" id="toc-the-training-loop" class="nav-link" data-scroll-target="#the-training-loop">The training loop</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<center>
<img src="network.jpg" alt="alt text" title="" height="400" width="800">
<figcaption>
Photo by <a href="https://unsplash.com/@alinnnaaaa?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Alina Grubnyak</a> on <a href="https://unsplash.com/photos/ZiQkhI7417A?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a>
</figcaption>
</center>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>In our high school mathematics, we have learnt <em>Differential Calculus</em>, which has introduced the terms ‘differentiation’ and ‘derivative’.</p>
<p>The concept of differentiating a function has played a significant role in today’s deep learning algorithms among many of its useful real life applications.</p>
<p>In this post, I will revisit the core idea of computing derivatives by looking at the geometrical interpretation of it and simultaneously try to put some light to the idea that is used behind the efficient learning process of a neural network, also known as <em>backpropagation</em>.</p>
<p>This post is highly motivated by a series of lecture videos by Andrej Karpathy. I have given a reference to it at the end.</p>
</section>
<section id="differentiation" class="level1">
<h1>Differentiation</h1>
<p>The term <em>differentiation</em> refers to the process of applying a mathematical operator to a function <span class="math inline">f</span>, with respect to its input <span class="math inline">x</span> and the output of differentiation is referred to as <em>derivative</em> (sometimes <em>gradient</em>) and it’s denoted by <span class="math inline">\frac{df}{dx}</span>.</p>
<p>Now, let’s assume that there is a function <span class="math inline">f(.)</span> that can be differentiated w.r.t to its input <span class="math inline">x</span> i.e.&nbsp;derivative of <span class="math inline">f</span> exists at some point <span class="math inline">x</span> (in <span class="math inline">f</span>’s domain of definition).</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>There are functions, for which derivative does not exist at some point. For example, we cannot perform differentiation of <span class="math inline">y = |x|</span> at point <span class="math inline">x=0</span>.</p>
<p>Read about conditions for differentiability and other related concepts <a href="https://math.libretexts.org/Under_Construction/Purgatory/Book%3A_Active_Calculus_(Boelkins_et_al.)/01%3A_Understanding_the_Derivative/1.07%3A_Limits_Continuity_and_Differentiability">here</a>.</p>
</div>
</div>
<p>The process of differentiation may return <em>another function</em>, say, <span class="math inline">g()</span> or a <em>constant</em> depending on the structure of the function <span class="math inline">f</span>.</p>
<p>The definition of derivative of a function <span class="math inline">f(.)</span> w.r.t the input <span class="math inline">x</span> is given below:</p>
<p><span class="math display">
\frac{df}{dx} = \lim_{h \to 0} \frac{f(x+h) - f(x)}{(x+h) - x}
</span></p>
<p>It’s a ratio, where the denominator measures the change in the input <span class="math inline">x</span> and the numerator measures the change in output <span class="math inline">f(x)</span>, hence indicating the rate of change in <span class="math inline">f</span> w.r.t <span class="math inline">x</span>.</p>
<p>The animation below, made with <a href="https://www.geogebra.org">geogebra</a>, will make the definition easier to understand by giving geometrical sense, especially the limiting nature of the ratio.</p>
<iframe src="https://www.geogebra.org/calculator/jbdd9y5v?embed" width="800" height="600" allowfullscreen="" style="border: 1px solid #e4e4e4;border-radius: 4px;" frameborder="0">
</iframe>
<p>Once you open this animation in full screen mode, you get to see a slider that is controlling our <span class="math inline">h</span>. On the left panel, there is a play button associated with <span class="math inline">h</span> and it can be used to play the animation.</p>
<p>If you make <span class="math inline">h</span> very close to zero, you will understand how the secant line passing through the points <span class="math inline">P</span> and <span class="math inline">Q</span> gradually becomes the tangent line of the curve at point <span class="math inline">A</span>.</p>
<p>As you understand, the ratio <span class="math inline">\frac{f(x+h) - f(x)}{(x+h) - x}</span> is nothing but the <em>slope</em> or <span class="math inline">tan(.)</span> of the angle <span class="math inline">\angle{QPP'}</span> from the triangle <span class="math inline">\triangle{QPP'}</span>.</p>
<p>Theoretically, when <span class="math inline">h</span> is very close to zero, the ratio gives you the value of the slope that the tangent line of the curve has, at the point <span class="math inline">(x,0)</span>.</p>
<p>Therefore, the overall definition of derivative of a function, geometrically refers to the slope (<span class="math inline">m</span>) for the tangent line of the curve <span class="math inline">y = f(x)</span> at the point <span class="math inline">x</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>A tangent line is a straight line that can be represented by the general equation of straight line <span class="math inline">y = mx+c</span>. Here <span class="math inline">m</span> is the outcome of the <span class="math inline">tan(.)</span> function of the angle that the tangent line makes with the x-axis. <span class="math inline">m</span> is called <em>slope</em> or <em>gradient</em> and <span class="math inline">c</span> is the intercept that the straight line makes with the y-axis.</p>
</div>
</div>
<p>The tangent line of a curve at some point <span class="math inline">x</span> is important as it gives us an idea about the nature of the curve locally. Geometrically, if the tangent line makes an acute angle with the x-axis (which is the case here), then it denotes a positive change on the output and when it makes an obtuse angle, it denotes a negative change on the output. If the tangent line is horizontal to the x-axis, then there is no change on the output.</p>
<p>Similar information can also be obtained by evaluating the expression of derivative at some point <span class="math inline">x</span>. If the limiting value of the ratio is positive, then obviously, there is a positive change on the output otherwise the change is negative and having a value as zero indicates there is no change on the output.</p>
<p>Note that, till now, we have discussed only on a scalar valued function with a scalar valued input. We will now try to get some ideas to calculate derivatives when either of the function and input or both of them are vector valued.</p>
<section id="derivative-for-different-type-of-input-mappings" class="level2">
<h2 class="anchored" data-anchor-id="derivative-for-different-type-of-input-mappings">Derivative for different type of input mappings</h2>
<section id="functions-of-type-f-mathbbrn-to-mathbbr---vector-in-scalar-out" class="level3">
<h3 class="anchored" data-anchor-id="functions-of-type-f-mathbbrn-to-mathbbr---vector-in-scalar-out">Functions of type <span class="math inline">f: \mathbb{R}^n \to \mathbb{R}</span> - vector in, scalar out</h3>
<p>Such functions take vector (s) as input (s) and return a scalar. The mean-squared-error and cross-entropy loss functions are a couple of examples of such functions that are heavily used in neural network setup.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
mean squared error (mse) loss for regression setup
</div>
</div>
<div class="callout-body-container callout-body">
<p>Suppose you have a continuous target variable (a.k.a dependent variable) <span class="math inline">Y</span> corresponding to a feature (a.k.a independent variable) <span class="math inline">X</span> for a regression problem.</p>
<p>Let us take <span class="math inline">N</span> as the total number of such pairs (i.e.&nbsp;examples) <span class="math inline">(x_i, y_i)_{i=1}^N</span> in our data. Now, at the end of the learning process of a neural network, we are about to get <span class="math inline">N</span> predictions which is another set of values <span class="math inline">\hat{y}_{i=1}^N</span>.</p>
<p>To measure the goodness of fit of the predictions w.r.t to actual target values (<span class="math inline">y_i</span> ’s), typically a mse loss function is used. This is defined as below:</p>
<p><span class="math display">
mse(y, \hat{y}) = \frac{1}{N} \sum_{i=1}^N (y_i - \hat{y}_i)^2
</span></p>
<p>We expect the mse value to be close to zero (or some predefined threshold depending on the use case and problem setup) to ensure the correctness of the prediction.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
cross entropy loss for classification setup
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider a binary classification problem with <span class="math inline">X</span> as the single feature variable and <span class="math inline">Y</span> as the class labels which can take a value as either zero or one. This is also known as a <em>binary logistic regression</em> problem.</p>
<p>As before, we have a total of <span class="math inline">N</span> examples <span class="math inline">(x_i, y_i)_{i=1}^N</span> in our data, where <span class="math inline">\forall i, y_i \in \left\{0,1\right\}</span>.</p>
<p>Binary classification is the simplest form of classification problem, which will help us to understand the concept easily.</p>
<p>Since, <span class="math inline">Y</span> can take only two possible values, it’s safe to consider that <span class="math inline">Y</span> is having a <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution"><em>bernoulli distribution</em></a>.</p>
<p>Also assume,</p>
<p><span class="math display">\begin{align}
  Y &amp;= 1 \Rightarrow \text{success} \nonumber \\
    &amp;= 0 \Rightarrow \text{failure} \nonumber
\end{align}</span></p>
<p>Here we need one more quantity <span class="math inline">p</span> as probability of having a success as part of the distribution of <span class="math inline">Y</span>, it is an important attribute of the data that we have.</p>
<p>Now, following the nature of bernoulli distribution, the probability of <span class="math inline">Y</span> taking an outcome, can be expressed in a generic way as given below:</p>
<p><span class="math display">
P(Y = y) = p^y \times (1-p)^{1-y}; \text{where } Y \in \left\{{0,1}\right\}
</span></p>
<p>Note that, putting <span class="math inline">y=1</span> in the above function will give you <span class="math inline">p</span>. This function always gives a value in between zero and one. It is also known as the <em>likelihood function</em> given that the quantity <span class="math inline">p</span> is unknown, but <span class="math inline">y</span> is known.</p>
<p>Having defined the above expression, we are ready to look at the entire data. It is to be noted that each of the examples are independent of each other.</p>
<p>Now, the next step is to find out the probability of having the target values themselves as <span class="math inline">y_1, y_2, \ldots, y_N</span> at some point of time that we are considering for the classification task.</p>
<p>To be specific, we would want to know that whichever process has generated the data, what would be the probability of generating this sample again. This combined probability is called <em>likelihood</em>.</p>
<p>Since these <span class="math inline">Y_i</span>’s are independent of each other, this likelihood (equivalent to joint probability) of <span class="math inline">Y_1=y_1, \dots, Y_N=y_N</span> is just the product of the individual likelihoods i.e.&nbsp;</p>
<p><span class="math display">\begin{align}
  P(Y_1=y_1, \dots, Y_N=y_N) &amp;= \prod_{i=1}^N P(Y_i = y_i) \nonumber \\
                              &amp;= \prod_{i=1}^N p_i^{y_i} \times (1-p_i)^{1-y_i} \nonumber \\
\end{align}</span></p>
<p>At this stage, a log function is typically applied to scale the above quantity for future calculations. The log function has same pattern as the function on which it is applied to i.e.&nbsp;mathematically speaking,</p>
<p><span class="math display">
\text{if } \forall a,b \in \mathbb{R}, a &gt; b  \Rightarrow f(a) &gt; f(b), \text{ then } log(f(a)) &gt; log(f(b))
</span></p>
<p>Therefore we have, <span class="math display">\begin{align}
  log\left[P(Y_1=y_1, \dots, Y_N=y_N)\right] &amp;= \sum_{i=1}^N y_i log(p_i) + (1-y_i) log(1-p_i) \nonumber
\end{align}</span></p>
<p>The above quantity is a negative quantity as the log function returns negative values when the input is within zero and one and working with a negative quantity can be conceptually misleading. That is why it is usually multiplied by negative one to make it positive.</p>
<p>This positive quantity is often referred to as <em>negative log likelihood (nll)</em> and is considered as the loss function for this type of binary classification problems.</p>
<p>Hence, the <em>negative log-likelihood</em> loss function is finally expressed as,</p>
<p><span class="math display">
nll = -\sum_{i=1}^N \left\{y_i log(p_i) + (1-y_i) log(1-p_i)\right\}
</span></p>
<p>It should be noted that, the quantity <span class="math inline">p_i</span> is usually estimated by the model (like a FFN for binary classification or logistic regression), whereas <span class="math inline">y_i</span> being the true value in our data for the <span class="math inline">i^{th}</span> sample.</p>
<p>The NLL loss function tries to measure the similarity between the distribution of the predicted values <span class="math inline">p</span> and the distribution of actual values <span class="math inline">y</span> (i.e.&nbsp;labels).</p>
<p>The <a href="https://en.wikipedia.org/wiki/Cross_entropy"><em>cross entropy</em></a> loss function is just a generalisation of this simple negative log likelihood giving the similar kind of information for a prediction when the data has, say, <span class="math inline">K</span> labels instead of just two.</p>
<p>It is expressed as,</p>
<p><span class="math display">
\text{cross entropy loss} = - \sum_{k=1}^K y_k log(\hat{y}_k)
</span></p>
<p>Where <span class="math inline">y_k</span>’s are true probabilities (or give the true distribution of labels), whereas <span class="math inline">\hat{y}_k</span>’s are predicted probabilities (or give the predicted/estimated distribution of labels).</p>
</div>
</div>
<p>The vector of gradients of such a function <span class="math inline">f</span> is defined as below:</p>
<p><span class="math display">
\frac{df}{d\vec{x}} = \left(\frac{df}{dx_1}, \ldots, \frac{df}{dx_n}\right)'
</span></p>
</section>
<section id="functions-of-type-f-mathbbrn-to-mathbbrm---vector-in-vector-out" class="level3">
<h3 class="anchored" data-anchor-id="functions-of-type-f-mathbbrn-to-mathbbrm---vector-in-vector-out">Functions of type <span class="math inline">f: \mathbb{R}^n \to \mathbb{R}^m</span> - vector in, vector out</h3>
<p>Such functions take vector (s) as input (s) and return another vector with different dimensions as output.</p>
<p>A simple example can be a function <span class="math inline">f: \mathbb{R}^2 \to \mathbb{R}^3</span>:</p>
<ul>
<li>input: <span class="math inline">\vec{x} = \left(x_1, x_2\right)</span> and a matrix of parameters <span class="math inline">\mathbf{W}_{3 \times 2}</span></li>
<li>output: <span class="math inline">\vec{y} = f(\vec{x}) = \left(w_{11}x_1+w_{12}x_2, w_{21}x_1+w_{22}x_2, w_{31}x_1+w_{32}x_2\right)</span></li>
</ul>
<p>In this case, derivative of <span class="math inline">f(\vec{x})</span> will be a matrix of dimension <span class="math inline">m \times n</span>, as given below:</p>
<p><span class="math display">\begin{align}
    \frac{d\vec{y}}{d\vec{x}} &amp;= \left(\frac{d\vec{y}}{dx_1}, \ldots, \frac{d\vec{y}}{dx_n}\right) \nonumber \\
    &amp;= \begin{pmatrix}
            \frac{dy_1}{dx_1} &amp; \frac{dy_1}{dx_2} &amp; \ldots &amp; \frac{dy_1}{dx_n} \\
            \frac{dy_2}{dx_1} &amp; \frac{dy_2}{dx_2} &amp; \ldots &amp; \frac{dy_2}{dx_n} \\
            \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
            \frac{dy_m}{dx_1} &amp; \frac{dy_m}{dx_2} &amp; \ldots &amp; \frac{dy_m}{dx_n} \\
        \end{pmatrix} \nonumber
\end{align}</span></p>
<p>This matrix is often referred to as <a href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant"><em>jacobian matrix</em></a> of <span class="math inline">\vec{y}</span> with respect to <span class="math inline">\vec{x}</span>.</p>
</section>
</section>
<section id="the-chain-rule-of-differentiation" class="level2">
<h2 class="anchored" data-anchor-id="the-chain-rule-of-differentiation">The chain rule of differentiation</h2>
<p>Consider the situation, where you have two different functions <span class="math inline">f(.)</span> and <span class="math inline">g(.)</span> defined as follows <span class="math inline">y = f(x)</span> and <span class="math inline">z = g(y) = g\left(f(x)\right)</span>.</p>
<p>While forming <span class="math inline">z</span>, <span class="math inline">f(.)</span> is the inner function and <span class="math inline">g(.)</span> is the outer function. This is known as function composition as often denoted as <span class="math inline">g \circ f(x)</span>.</p>
<p>We want to measure the rate of change in <span class="math inline">z</span> w.r.t <span class="math inline">x</span> i.e.&nbsp;<span class="math inline">\frac{dz}{dx}</span>.</p>
<p>This derivative is computed as follows:</p>
<p><span class="math display">
\frac{dz}{dx} = \frac{dz}{dy} \times \frac{dy}{dx}
</span></p>
<p>and it is known as the <em>chain rule</em> of differentiation.</p>
<p>Here, the actual derivative is computed in two steps,</p>
<ul>
<li>the local derivative of <span class="math inline">z</span> w.r.t <span class="math inline">y</span> as <span class="math inline">z</span> explicitly depends on <span class="math inline">y</span></li>
<li>the local derivative of <span class="math inline">y</span> w.r.t <span class="math inline">x</span> as <span class="math inline">y</span> explicitly depends on <span class="math inline">x</span></li>
</ul>
<p>and these are multiplied together to form the actual derivative.</p>
<p><a href="https://en.wikipedia.org/wiki/Chain_rule">Wikipedia</a> has an intuitive explanation to this as given below:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Understanding chain rule
</div>
</div>
<div class="callout-body-container callout-body">
<p>Intuitively, the chain rule states that knowing the instantaneous rate of change of <span class="math inline">z</span> relative to <span class="math inline">y</span> and that of <span class="math inline">y</span> relative to <span class="math inline">x</span> allows one to calculate the instantaneous rate of change of <span class="math inline">z</span> relative to <span class="math inline">x</span> as the product of the two rates of change.</p>
<p>As put by George F. Simmons: “if a car travels twice as fast as a bicycle and the bicycle is four times as fast as a walking man, then the car travels 2 × 4 = 8 times as fast as the man.”</p>
</div>
</div>
<p>This chain rule is very important as it does all the heavy lifting for backpropagation.</p>
</section>
<section id="partial-differentiation" class="level2">
<h2 class="anchored" data-anchor-id="partial-differentiation">Partial differentiation</h2>
<p>Partial differentiation is mainly applicable for a function that depends on several input variables simultaneously.</p>
<p>For example, we might have a function like <span class="math inline">f(x,y,z) = ax+by+cz+d</span> representing a plane in a three dimensional coordinate system.</p>
<p>If we would like to measure the rate of change only along the x-axis, we would perform partial differentiation w.r.t <span class="math inline">x</span> and by ignoring other inputs.</p>
<p>Unlike the previous one (i.e.&nbsp;complete derivative), partial derivative is denoted by <span class="math inline">\frac{\partial f}{\partial x}</span> and the definition is given below:</p>
<p><span class="math display">
\frac{\partial f}{\partial x} = \lim_{h \to 0} \frac{f(x+h,y,z) - f(x,y,z)}{(x+h) - x}
</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the neural network setup, we usually deal with complex functions that depend on several variables. Hence, it will be meaningful to use partial derivatives while demystifying the underlying operations.</p>
</div>
</div>
</section>
</section>
<section id="the-computational-graph" class="level1">
<h1>The computational graph</h1>
<p>A <em>computational graph</em> is a directed acyclic graph that keeps track of the sequence of operations performed during the run time of an algorithm.</p>
<p>Let us take an example to understand it.</p>
<div class="cell" data-fig-width="4" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
  a("$$a$$") --&gt; c("$$c = a+1$$")
  a("$$a$$") --&gt; d("$$d = a+b$$")
  b("$$b$$") --&gt; d("$$d = a+b$$")
  c("$$c = a+1$$") --&gt; e("$$e = c \times d$$")
  d("$$d = a+b$$") --&gt; e("$$e = c \times d$$")
  e("$$e = c \times d$$") --&gt; l("$$l = e+f$$")
  f("$$f$$") --&gt; l("$$l = e+f$$")
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>The above diagram is typically referred to as a computational graph, where both <strong>a</strong> and <strong>b</strong> are inputs to the graph and <span class="math inline">l</span> is the output of the graph.</p>
<p>Each box in the graph is called a <em>node</em> where a mathematical operation is performed. So, if you imagine, a deep neural network is actually such a massive computational graph where, depending on the depth of the network, millions (or maybe billions) of mathematical operations are performed.</p>
<p>A partial view of a graph is shown below so that we can imagine how large and complex a computational graph can be for a real neural network:</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
  x1("$$x_1$$") --&gt; x1w11("$$x_1 \times w_{11}$$")
  w11("$$w_{11}$$") --&gt; x1w11("$$x_1 \times w_{11}$$")

  x2("$$x_2$$") --&gt; x2w12("$$x_2 \times w_{12}$$")
  w12("$$w_{12}$$") --&gt; x2w12("$$x_2 \times w_{12}$$")

  x1w11("$$x_1 \times w_{11}$$") --&gt; x1w11x2w12("$$(x_1 \times w_{11}) + (x_2 \times w_{12})$$")
  x2w12("$$x_2 \times w_{12}$$") --&gt; x1w11x2w12("$$(x_1 \times w_{11}) + (x_2 \times w_{12})$$")

  x1w11x2w12("$$(x_1 \times w_{11}) + (x_2 \times w_{12})$$") --&gt; x1w11x2w12b1("$$(x_1 \times w_{11}) + (x_2 \times w_{12}) + b_1$$")
  b1("$$b_1$$") --&gt; x1w11x2w12b1("$$(x_1 \times w_{11}) + (x_2 \times w_{12}) + b_1$$")

  x1w11x2w12b1("$$(x_1 \times w_{11}) + (x_2 \times w_{12}) + b_1$$") --&gt; sigmoid1("$$\sigma_1$$")

  x1("$$x_1$$") --&gt; x1w21("$$x_1 \times w_{21}$$")
  w21("$$w_{21}$$") --&gt; x1w21("$$x_1 \times w_{21}$$")

  x2("$$x_2$$") --&gt; x2w22("$$x_2 \times w_{22}$$")
  w22("$$w_{22}$$") --&gt; x2w22("$$x_2 \times w_{22}$$")

  x1w21("$$x_1 \times w_{21}$$") --&gt; x1w21x2w22("$$(x_1 \times w_{21}) + (x_2 \times w_{22})$$")
  x2w22("$$x_2 \times w_{22}$$") --&gt; x1w21x2w22("$$(x_1 \times w_{21}) + (x_2 \times w_{22})$$")

  x1w21x2w22("$$(x_1 \times w_{21}) + (x_2 \times w_{22})$$") --&gt; x1w21x2w22b2("$$(x_1 \times w_{21}) + (x_2 \times w_{22}) + b_2$$")
  b2("$$b_2$$") --&gt; x1w21x2w22b2("$$(x_1 \times w_{21}) + (x_2 \times w_{22}) + b_2$$")

  x1w21x2w22b2("$$(x_1 \times w_{21}) + (x_2 \times w_{22}) + b_2$$") --&gt; sigmoid2("$$\sigma_2$$")

  sigmoid1("$$\sigma_1$$") --&gt; loss("$$loss$$")
  sigmoid2("$$\sigma_2$$") --&gt; loss("$$loss$$")
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<section id="differentiation-through-a-graph" class="level2">
<h2 class="anchored" data-anchor-id="differentiation-through-a-graph">Differentiation through a graph</h2>
<p>Neural networks learn (i.e.&nbsp;update the parameters) using an iterative process where at the core it uses backpropagation algorithm to compute derivatives of a loss function w.r.t each node in the graph.</p>
<p>Let us revisit the previously shown simple computational graph to understand it in a better way,</p>
<div class="cell" data-fig-width="4.5" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
  a("$$a$$") --&gt; c("$$c = a+1$$")
  a("$$a$$") --&gt; d("$$d = a+b$$")
  b("$$b$$") --&gt; d("$$d = a+b$$")
  c("$$c = a+1$$") --&gt; e("$$e = c \times d$$")
  d("$$d = a+b$$") --&gt; e("$$e = c \times d$$")
  e("$$e = c \times d$$") --&gt; l("$$l = e+f$$")
  f("$$f$$") --&gt; l("$$l = e+f$$")
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>Here, we can assume <span class="math inline">l</span> as the <em>loss</em> and we want to measure the rate of change of loss w.r.t each of the nodes (including loss node itself) in the backward direction (why backward? … I will explain later).</p>
<p>This whole computation is performed in multiple steps as follows:</p>
<ul>
<li>Step 1: rate of change of <span class="math inline">l</span> w.r.t <span class="math inline">l</span> itself i.e.&nbsp;<span class="math inline">\frac{\partial l}{\partial l} = 1</span></li>
<li>Step 2: rate of change of <span class="math inline">l</span> w.r.t <span class="math inline">e</span> i.e.&nbsp;<span class="math inline">\frac{\partial l}{\partial e} = \frac{\partial l}{\partial e} \times \frac{\partial l}{\partial l} = \frac{\partial (e+f)}{\partial e} \times \frac{\partial l}{\partial l} = 1</span></li>
<li>Step 3: rate of change of <span class="math inline">l</span> w.r.t <span class="math inline">f</span> i.e.&nbsp;<span class="math inline">\frac{\partial l}{\partial f} = \frac{\partial l}{\partial f} \times \frac{\partial l}{\partial l} = \frac{\partial (e+f)}{\partial f} \times \frac{\partial l}{\partial l} = 1</span></li>
<li>Step 4: rate of change of <span class="math inline">l</span> w.r.t <span class="math inline">c</span> i.e.&nbsp;<span class="math inline">\frac{\partial l}{\partial c} = \frac{\partial e}{dc} \times \frac{\partial l}{\partial e} = \frac{\partial (c*d)}{\partial c} \times \frac{\partial l}{\partial e} = d</span></li>
<li>Step 5: rate of change of <span class="math inline">l</span> w.r.t <span class="math inline">d</span> i.e.&nbsp;<span class="math inline">\frac{\partial l}{\partial d} = \frac{\partial e}{\partial d} \times \frac{\partial l}{\partial e} = \frac{\partial (c*d)}{\partial d} \times \frac{\partial l}{\partial e} = c</span></li>
<li>Step 6: rate of change of <span class="math inline">l</span> w.r.t <span class="math inline">a</span> i.e.&nbsp;<span class="math inline">\frac{\partial l}{\partial a}</span>, this step can be broken into two parts as changing <span class="math inline">a</span>, can change <span class="math inline">l</span> either through <span class="math inline">c</span> or through <span class="math inline">d</span>, therefore,
<ul>
<li><span class="math inline">\frac{\partial l}{\partial a} = \underbrace{\left( \frac{\partial c}{\partial a} \times \frac{\partial l}{\partial c} \right)}_{\text{changes through c}} + \underbrace{\left( \frac{\partial d}{\partial a} \times \frac{\partial l}{\partial d} \right)}_{\text{changes through d}} = d + c</span></li>
</ul></li>
<li>Step 7: rate of change of <span class="math inline">l</span> w.r.t <span class="math inline">b</span> i.e.&nbsp;<span class="math inline">\frac{\partial l}{\partial b} = \frac{\partial d}{\partial b} \times \frac{\partial l}{\partial d} = 1 \times c = c</span></li>
</ul>
<p>The real benefit of computing derivatives (a.k.a gradients) in the backward direction is that it allows the learning algorithm to compute gradients for all the nodes with a single attempt.</p>
<p>If we would have started in the forward direction, then the learning algorithm would have travelled the entire graph for each of the input nodes at the very beginning. Unlike a simple graph like the above one, this forward mode differentiation is computationally very costly for a large network.</p>
<p><a href="https://pytorch.org">Pytorch</a>, being a popular deep learning toolkit, has implemented a <code>.backward()</code> method for their neural network class to compute the derivatives for all the nodes (technically not all, only for them with <code>requires_grad=True</code>) in a computational graph with a single attempt.</p>
</section>
<section id="important-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="important-takeaways">Important takeaways</h2>
<p>When we have a <span class="math inline">'+'</span> operation in the graph, like the one below,</p>
<div class="cell" data-fig-width="3.5" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph RL
  subgraph input
  x("$$x$$")
  y("$$y$$")
  end

  subgraph output
  z("$$z = x+y$$")
  end

  x("$$x$$") --&gt; z("$$z = x+y$$")-.-&gt;|gradient of z|x("$$x$$")
  y("$$y$$") --&gt; z("$$z = x+y$$")-.-&gt;|gradient of z|y("$$y$$")
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>Computing gradients with respect to the input nodes is fairly simple as the backward gradient computation just copies the gradient of the output node to all the input nodes.</p>
<p>However, when we have a <span class="math inline">'\times'</span> operation in the graph, like the next one,</p>
<div class="cell" data-fig-width="3.5" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph RL
  x("$$x$$") --&gt; z("$$z = x \times y$$")-.-&gt;|value of y * gradient of z|x("$$x$$") 
  y("$$y$$") --&gt; z("$$z = x \times y$$")-.-&gt;|value of x * gradient of z|y("$$y$$")

  subgraph input
  x("$$x$$")
  y("$$y$$")
  end

  subgraph output
  z("$$z = x \times y$$")
  end
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>The gradient of one input node is just the product of the gradient of the output node and the value of the other input node.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math inline">+</span> and <span class="math inline">\times</span> are the two building blocks powering up any mathematical computation.</p>
</div>
</div>
</section>
<section id="sorting-nodes-before-performing-backpropagation" class="level2">
<h2 class="anchored" data-anchor-id="sorting-nodes-before-performing-backpropagation">Sorting nodes before performing backpropagation</h2>
<div class="cell" data-fig-width="3.5" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
  a("$$a$$") --&gt; c("$$c = a + b$$")
  b("$$b$$") --&gt; c("$$c = a + b$$")
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>If we observe the above graph, it is clear that while performing backpropagation, computing gradient of any of the leaf nodes i.e.&nbsp;<span class="math inline">\left\{a, b\right\}</span> is possible only when gradient value for the node <span class="math inline">c</span> is already available. This is just because of the chain rule we use.</p>
<p>So the fact is that we cannot choose a node randomly and try computing the gradient for it. We must compute the gradient of node <span class="math inline">c</span>, which is essentially 1, then we can take either of node <span class="math inline">a</span> or node <span class="math inline">b</span>.</p>
<p>Nodes must be sorted in order to perform backpropagation. <a href="https://www.hackerearth.com/practice/algorithms/graphs/topological-sort/tutorial/"><em>Topological Sort</em></a> is a way to achieve this ordering before performing the backpropagation.</p>
</section>
<section id="the-efficient-vector-jacobian-product" class="level2">
<h2 class="anchored" data-anchor-id="the-efficient-vector-jacobian-product">The efficient vector-jacobian product</h2>
<p>Consider our tiny network:</p>
<div class="cell" data-fig-width="3.5" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
  x1("$$x_1$$")--&gt;|"$$w_{11}$$"|y1("$$y_1$$")
  x2("$$x_2$$")--&gt;|"$$w_{12}$$"|y1("$$y_1$$")
  x1("$$x_1$$")--&gt;|"$$w_{21}$$"|y2("$$y_2$$")
  x1("$$x_2$$")--&gt;|"$$w_{22}$$"|y2("$$y_2$$")
  x1("$$x_1$$")--&gt;|"$$w_{31}$$"|y3("$$y_3$$")
  x2("$$x_2$$")--&gt;|"$$w_{32}$$"|y3("$$y_3$$")
  y1("$$y_1$$")--&gt;l("$$l$$")
  y2("$$y_2$$")--&gt;l("$$l$$")
  y3("$$y_3$$")--&gt;l("$$l$$")

  subgraph "input layer"
  x1("$$x_1$$") 
  x2("$$x_2$$")
  end

  subgraph "hidden layer"
  y1("$$y_1$$")
  y2("$$y_2$$")
  y3("$$y_3$$")
  end

  subgraph "output layer"
  l("$$l$$")
  end
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>It has only two input nodes, a single output node and in between them three additional nodes forming the hidden layer.</p>
<p>The mapping from input layer to hidden layer is supported by a matrix of parameters <span class="math inline">\mathbf{W}_{3 \times 2}</span> and of course by a vector valued linear function, say, <span class="math inline">\vec{f}</span>.</p>
<p>Hidden layer values are formed using the equation below:</p>
<p><span class="math display">
\begin{pmatrix}
  y_1 \\
  y_2 \\
  y_3
\end{pmatrix} = \begin{pmatrix}
                  w_{11} &amp; w_{12} \\
                  w_{21} &amp; w_{22} \\
                  w_{31} &amp; w_{32}
                \end{pmatrix} \begin{pmatrix}
                                x_1 \\
                                x_2
                              \end{pmatrix}
</span></p>
<p>where <span class="math inline">w_{ij}; i=1(1)3, j=1(1)2</span> is the weight for the connection coming from <span class="math inline">j^{th}</span> node in the input layer to <span class="math inline">i^{th}</span> neuron in the hidden layer and consider a differentiable scalar valued function <span class="math inline">g(.)</span> that combines hidden layer output values to form the final output <span class="math inline">l</span>.</p>
<p>Computing <span class="math inline">l</span> from the given fixed set of input values and with the help of some values of the parameters, is called the <em>forward pass</em>.</p>
<p>At this stage, we can definitely compute the jacobian matrix of <span class="math inline">\vec{f}</span> w.r.t <span class="math inline">\vec{x}</span>, which is given below:</p>
<p><span class="math display">
\mathbf{J} = \begin{pmatrix}
                \frac{\partial y_1}{\partial x_1} &amp; \frac{\partial y_1}{\partial x_2} \\
                \frac{\partial y_2}{\partial x_1} &amp; \frac{\partial y_2}{\partial x_2} \\
                \frac{\partial y_3}{\partial x_1} &amp; \frac{\partial y_3}{\partial x_2}
              \end{pmatrix}
</span></p>
<p>Now, suppose <span class="math inline">\vec{v}</span> is the gradient vector of <span class="math inline">l</span> i.e.&nbsp;<span class="math inline">\vec{v} = \left(\frac{dl}{dy_1}, \frac{dl}{dy_2}, \frac{dl}{dy_3}\right)'</span> and we will assume that we have already computed it. Strange? No problem, I will explain this assumption at the end of this section.</p>
<p>Now, if we take the dot product of <span class="math inline">\mathbf{J}^T</span> and <span class="math inline">\vec{v}</span>, this is what we are going to get,</p>
<p><span class="math display">\begin{align}
\mathbf{J}^T \cdot \vec{v} &amp;= \begin{pmatrix}
                                \frac{\partial y_1}{\partial x_1} &amp; \frac{\partial y_2}{\partial x_1} &amp; \frac{\partial y_3}{\partial x_1} \\
                                \frac{\partial y_1}{\partial x_2} &amp; \frac{\partial y_2}{\partial x_2} &amp; \frac{\partial y_3}{\partial x_2} \\
                              \end{pmatrix} \begin{pmatrix}
                                  \frac{\partial l}{\partial y_1} \\
                                  \frac{\partial l}{\partial y_2} \\
                                  \frac{\partial l}{\partial y_3} \\
                              \end{pmatrix} \nonumber \\
                           &amp;= \begin{pmatrix}
                                  \frac{\partial l}{\partial x_1} \\
                                  \frac{\partial l}{\partial x_2}
                              \end{pmatrix} \nonumber \\
                           &amp;= \frac{\partial l}{\partial \vec{x}} \nonumber
\end{align}</span></p>
<p>If we observe the dot product carefully, we will notice a series of chain rules being performed.</p>
<p>Therefore, this vector-jacobian product helps us to compute the gradient of loss function w.r.t the nodes in another layer which, in reality, might be far behind the output layer in the computational graph and it does so with the help of a known gradient vector for the layer right next to it.</p>
<p>The intermediate gradient vector has been considered to be computed beforehand because while performing reverse mode differentiation, we must know the gradient of the output node before computing the gradients of the input nodes which have defined the output node itself and we have seen it <a href="#derivative-through-a-graph">here</a> where we have started taking derivatives from the output node.</p>
<p>Here <span class="math inline">\vec{y}</span> is the output of <span class="math inline">f(\vec{x})</span>, so we can safely take this assumption to understand the theory.</p>
<p>Now, the next part is to compute <span class="math inline">\frac{\partial l}{\partial \mathbf{W}}</span> which can again be done using,</p>
<p><span class="math display">
\frac{\partial l}{\partial \mathbf{W}} = \frac{\partial \vec{y}}{\partial \mathbf{W}} \times \frac{\partial l}{\partial \vec{y}}
</span></p>
<p>Hopefully, it gives us an idea to implement automatic differentiation (autograd) for a large computational graph.</p>
</section>
<section id="the-training-loop" class="level2">
<h2 class="anchored" data-anchor-id="the-training-loop">The training loop</h2>
<p>The last and one of the important parts of training a neural network is the <em>training loop</em>. Technically it is a <code>for</code> loop where in each iteration some steps are performed. Each of these iterations is also called <em>epoch</em>.</p>
<p>It is to be noted that, before entering into the loop, all the parameters are randomly initialised.</p>
<p>Below are the steps that are performed within a training loop.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD
    A(randomly initialize&lt;br&gt;parameters) --&gt; B(compute output&lt;br&gt;for all the nodes&lt;br&gt;in each of the layers)--&gt;|compare with&lt;br&gt;ground truth|C(compute loss) --&gt; D(perform&lt;br&gt;backpropagation) --&gt; E(update&lt;br&gt;parameters) --&gt; B

    subgraph forward pass
    B
    C
    end

    subgraph backward pass
    D
    end

    subgraph update
    E
    end
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>In conclusion, I hope that this post has provided you with valuable insights and information about the learning process of a neural network.</p>
<p>Remember that learning is a never-ending process, and there is always more to discover and explore. I have created a <a href="https://colab.research.google.com/drive/1GgqYuomhrSgXjLJi01soTX-0h6RU7F0J?usp=sharing">colab notebook</a> based on Andrej’s lecture on YouTube, which will definitely be helpful to grasp the whole idea.</p>
<p>If you have any questions or feedback, please feel free to leave a comment or reach out to me directly.</p>
<p>Thank you for taking the time to read this blog, and I hope to see you again soon.</p>
</section>
<section id="references" class="level1">
<h1>References</h1>
<ul>
<li><a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">Neural Networks: Zero to Hero</a> by Andrej Karpathy</li>
<li><a href="https://colah.github.io/posts/2015-08-Backprop/">Calculus on Computational Graphs: Backpropagation</a></li>
<li><a href="http://cs231n.stanford.edu/handouts/derivatives.pdf">Derivatives, Backpropagation, and Vectorization</a></li>
<li><a href="https://youtu.be/eL-KzMXSXXI">Topological Sort Algorithm | Graph Theory</a></li>
<li><a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html">A Gentle Introduction To torch.autograd</a></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/koushikkhan\.github\.io\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<script src="https://utteranc.es/client.js" repo="koushikkhan/blog-discussions" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->




</body></html>